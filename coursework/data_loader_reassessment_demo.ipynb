{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794243c0",
   "metadata": {},
   "source": [
    "# Text Analytics Coursework -- Data Loader\n",
    "\n",
    "This notebook is to help get you started with the datasets used in the coursework assignment. \n",
    "\n",
    "For this coursework, we recommend that you use your virtual environment that you created for the labs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use HuggingFace's datasets library to access the Emotion dataset\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e90e82-bbdb-4532-91b1-58229d4f1043",
   "metadata": {},
   "source": [
    "# Financial News Sentiment Classification\n",
    "\n",
    "The Financial Phrasebank dataset contains sentences from financial news articles, classified into negative (0), neutral (1), or positive (2) sentiment. See [HuggingFace](https://huggingface.co/datasets/financial_phrasebank) for more information. \n",
    "\n",
    "First we need to load the data. We only have access to the original training data, so we will split this into our own validation and test sets. The _validation_ set (also called 'development' set or 'devset') can be used to compute performance of your model when tuning hyperparameters, optimising combinations of features, or looking at the errors your model makes before improving it. This allows you to hold out the test set (i.e., not to look at it at all when developing your method) to give a fair evaluation of the model and how well it generalises to new examples. This avoids tuning the model to specific examples in the test set. An alternative approach to validation is to not use a single fixed validation set, but instead use [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf1096-acce-4226-a172-5357f49e91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"financial_phrasebank\",\n",
    "    name=\"sentences_50agree\",\n",
    "    split=\"train\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "# Split the test set out\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_dataset[\"sentence\"], train_dataset[\"label\"], test_size=0.25)\n",
    "\n",
    "# Optionally, split out a validation set\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.25)\n",
    "\n",
    "print(f\"Training dataset with {len(train_labels)} instances loaded\")\n",
    "print(f\"Validation dataset with {len(val_labels)} instances loaded\")\n",
    "print(f\"Test dataset with {len(test_labels)} instances loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af0fbc-2c18-4b68-839d-802595dac600",
   "metadata": {},
   "source": [
    "# MIT Restaurants\n",
    "\n",
    "This dataset consists of requests for restaurant recommendations given by users of a dialogue system. They are tagged with named entities in eight categories:\n",
    "{\n",
    "    \"O\": 0,\n",
    "    \"B-Rating\": 1,\n",
    "    \"I-Rating\": 2,\n",
    "    \"B-Amenity\": 3,\n",
    "    \"I-Amenity\": 4,\n",
    "    \"B-Location\": 5,\n",
    "    \"I-Location\": 6,\n",
    "    \"B-Restaurant_Name\": 7,\n",
    "    \"I-Restaurant_Name\": 8,\n",
    "    \"B-Price\": 9,\n",
    "    \"B-Hours\": 10,\n",
    "    \"I-Hours\": 11,\n",
    "    \"B-Dish\": 12,\n",
    "    \"I-Dish\": 13,\n",
    "    \"B-Cuisine\": 14,\n",
    "    \"I-Price\": 15,\n",
    "    \"I-Cuisine\": 16\n",
    "}\n",
    "For further details, see [the HuggingFace page](https://huggingface.co/datasets/tner/mit_restaurant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2be06-2b36-4379-844a-ad32d0f718de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset = load_dataset(\n",
    "    \"tner/mit_restaurant\", \n",
    ")\n",
    "\n",
    "print(f'The dataset is a dictionary with {len(ner_dataset)} splits: \\n\\n{ner_dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515ff00-f8bc-42ac-801f-50224ea50e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It  may be useful to obtain the data in a list format for some sequence tagging methods\n",
    "train_sentences_ner = [item['tokens'] for item in ner_dataset['train']]\n",
    "train_labels_ner = [[str(tag) for tag in item['tags']] for item in ner_dataset['train']]\n",
    "\n",
    "val_sentences_ner = [item['tokens'] for item in ner_dataset['validation']]\n",
    "val_labels_ner = [[str(tag) for tag in item['tags']] for item in ner_dataset['validation']]\n",
    "\n",
    "test_sentences_ner = [item['tokens'] for item in ner_dataset['test']]\n",
    "test_labels_ner = [[str(tag) for tag in item['tags']] for item in ner_dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2cad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the different tag values in the dataset:\n",
    "np.unique(np.concatenate(train_labels_ner))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258fddc-2c05-43b2-870b-a2fe7fff0252",
   "metadata": {},
   "source": [
    "### (Optional) Transformer Sequence Tagger\n",
    "\n",
    "People that want to use a transformer for task 2 may want to take a look at the [Token Classification tutorial](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb#scrollTo=vc0BSBLIIrJQ) from HuggingFace. There is no requirement to use a transformer to achieve high marks, this is one option that you may consider. Feel free to skip this part of the notebook if you are using a different kind of model that does not require it.\n",
    "\n",
    "A useful function provided by HuggingFace as part of the Token Classification page is tokenize_and_align. You can reuse this function if you are working with a method that tokenizes the text in a diferent way to the NER dataset. This function is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07deb893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, max_length=128, is_split_into_words=True)\n",
    "    print(tokenized_inputs.keys())\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# An example of how to use tokenize_and_align:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\") \n",
    "label_all_tokens=False\n",
    "\n",
    "tokenized_dataset = ner_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_analytics",
   "language": "python",
   "name": "text_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
